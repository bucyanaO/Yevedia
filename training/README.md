# Yevedia Fine-Tuning System
# ===========================
# Syst√®me d'entra√Ænement MLX pour personnaliser Phi-3

## üéØ Vue d'ensemble

Ce syst√®me permet de fine-tuner le mod√®le Phi-3 avec vos propres donn√©es
pour cr√©er un assistant IA personnalis√© appel√© Yevedia.

## üìÅ Structure

```
training/
‚îú‚îÄ‚îÄ data/                  # Donn√©es d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl        # Exemples d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ valid.jsonl        # Exemples de validation
‚îÇ   ‚îî‚îÄ‚îÄ test.jsonl         # Exemples de test
‚îú‚îÄ‚îÄ models/                # Mod√®les entra√Æn√©s
‚îÇ   ‚îî‚îÄ‚îÄ adapters/          # Adaptateurs LoRA
‚îú‚îÄ‚îÄ scripts/               # Scripts Python
‚îÇ   ‚îú‚îÄ‚îÄ export_data.py     # Export des donn√©es depuis SQLite
‚îÇ   ‚îú‚îÄ‚îÄ finetune.py        # Script principal de fine-tuning
‚îÇ   ‚îî‚îÄ‚îÄ create_ollama_model.py  # Cr√©ation mod√®le Ollama
‚îî‚îÄ‚îÄ README.md              # Ce fichier
```

## üöÄ Utilisation rapide

### √âtape 1: Exporter les donn√©es

```bash
cd /Users/emci/Documents/App/Yevedia/training/scripts
python export_data.py
```

Cela va:
- Exporter vos conversations depuis la base de donn√©es
- Exporter vos m√©moires et documents
- Cr√©er les fichiers train.jsonl, valid.jsonl, test.jsonl

### √âtape 2: Lancer le fine-tuning

```bash
python finetune.py
```

Cela va:
- V√©rifier les d√©pendances (MLX, transformers, etc.)
- T√©l√©charger Phi-3 si n√©cessaire
- Lancer l'entra√Ænement LoRA
- Sauvegarder les adaptateurs

### √âtape 3: Utiliser le mod√®le

Apr√®s le fine-tuning, vous pouvez tester le mod√®le:

```bash
python -m mlx_lm.generate \
    --model microsoft/Phi-3-mini-4k-instruct \
    --adapter-path ./models/adapters/yevedia-lora \
    --prompt "Qui es-tu ?"
```

## ‚öôÔ∏è Configuration

Modifiez les param√®tres dans `finetune.py`:

| Param√®tre | Description | Valeur par d√©faut |
|-----------|-------------|-------------------|
| `lora_rank` | Rang LoRA (plus = plus pr√©cis mais plus lent) | 8 |
| `batch_size` | Taille du batch (r√©duire si manque de RAM) | 4 |
| `epochs` | Nombre d'√©poques | 3 |
| `learning_rate` | Taux d'apprentissage | 1e-5 |

## üìä Format des donn√©es

Les donn√©es doivent √™tre au format JSONL (une ligne JSON par exemple):

```json
{"instruction": "Question de l'utilisateur", "input": "", "output": "R√©ponse souhait√©e"}
```

Exemple:
```json
{"instruction": "Qui es-tu ?", "input": "", "output": "Je suis Yevedia, ton assistant IA personnel."}
{"instruction": "Quel temps fait-il ?", "input": "", "output": "Je n'ai pas acc√®s aux donn√©es m√©t√©o en temps r√©el, mais je peux t'aider avec d'autres questions."}
```

## üîß D√©pannage

### "Not enough memory"
R√©duisez `batch_size` √† 2 ou 1 dans `finetune.py`.

### "Model not found"
Le mod√®le sera t√©l√©charg√© automatiquement depuis Hugging Face.
Assurez-vous d'avoir une connexion internet.

### "No training data"
Ex√©cutez d'abord `export_data.py` ou ajoutez manuellement
des fichiers dans le dossier `data/`.

## üí° Conseils

1. **Plus de donn√©es = meilleur r√©sultat**
   - Minimum recommand√©: 50 exemples
   - Id√©al: 200+ exemples

2. **Qualit√© > Quantit√©**
   - Des exemples bien r√©dig√©s sont plus importants que beaucoup d'exemples m√©diocres

3. **Diversit√©**
   - Incluez diff√©rents types de questions/r√©ponses
   - Ajoutez des exemples de conversations normales et techniques

4. **It√©rations**
   - Commencez avec peu d'√©poques (2-3)
   - Augmentez si le mod√®le ne converge pas bien

## üñ•Ô∏è Sp√©cifications Mac recommand√©es

| RAM | Mod√®le support√© | Temps estim√© |
|-----|-----------------|--------------|
| 16 Go | Phi-3 (3B) | 15-30 min |
| 32 Go | Phi-3 + Mistral 7B | 20-45 min |
| 64 Go | Tous jusqu'√† 13B | 30-60 min |
| 96 Go | Tous jusqu'√† 70B (QLoRA) | 45-90 min |

Votre Mac de 96 Go peut entra√Æner pratiquement n'importe quel mod√®le! üöÄ
